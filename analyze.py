"""Wrapper around the OpenAI Python SDK to analyze PubMed abstracts.

This module provides a simple function for sending a batch of abstracts to
an OpenAI Chat completion endpoint. It trims the input based on the
model's approximate context window and constructs a prompt suitable for
identifying biotech opportunities in the literature.
"""

import os
from openai import OpenAI

__all__ = ["analyze_abstracts"]


# Approximate character caps per model to stay within context window
MODEL_CHAR_CAP = {
    "gpt-4o-mini": 80_000,
    "gpt-4o": 120_000,
}


def analyze_abstracts(abstracts_text: str, model: str = "gpt-4o-mini") -> str:
    """Send a series of abstracts to the OpenAI API for opportunity analysis.

    Parameters
    ----------
    abstracts_text : str
        Plain text containing one or more PubMed abstracts.
    model : str, optional
        The OpenAI Chat model to use (default is "gpt-4o-mini").

    Returns
    -------
    str
        The analysis response generated by the model.
    """
    # Initialize the OpenAI client with the API key from environment variables
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    # Ensure we don't exceed the model context by trimming the input
    cap = MODEL_CHAR_CAP.get(model, 80_000)
    snippet = abstracts_text[:cap]
    # Prepare system and user messages
    system_prompt = (
        "You are a biotech venture analyst. Given recent PubMed abstracts, "
        "identify potential pipeline expansion and newco opportunities. "
        "Summarize themes, modalities, targets, validation level, "
        "differentiation vs. standard of care, and partnering angles."
    )
    user_prompt = (
        "Analyze the following abstracts (focus on novelty, tractability, and commercial potential). "
        "Return: (1) bullet summary of themes; (2) ranked short-list of 3â€“5 opportunities with rationale.\n\n"
        f"{snippet}"
    )
    # Call the OpenAI chat completion endpoint
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        temperature=0.2,
    )
    # Extract and return the model's reply content
    return response.choices[0].message.content.strip()